{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting training data from the ODC\n",
    "\n",
    "* **Products used:** \n",
    "[gm_s2_annual](https://explorer.digitalearth.africa/gm_s2_annual)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Keywords** :index:`data used; sentinel-2 geomedian`, index:`data used; MADs`, :index:`data methods; machine learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Training data** is the most important part of any supervised machine learning workflow. The quality of the training data has a greater impact on the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy ([Maxell et al 2018](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343)).  A review of training data methods in the context of Earth Observation is available [here](https://www.mdpi.com/2072-4292/12/6/1034) \n",
    "\n",
    "When creating training labels, be sure to capture the **spectral variability** of the class, and to use imagery from the time period you want to classify (rather than relying on basemap composites). Another common problem with training data is **class imbalance**. This can occur when one of your classes is relatively rare and therefore the rare class will comprise a smaller proportion of the training set. When imbalanced data is used, it is common that the final classification will under-predict less abundant classes relative to their true proportion.\n",
    "\n",
    "There are many platforms to use for gathering training labels, the best one to use depends on your application. GIS platforms are great for collection training data as they are highly flexible and mature platforms; [Geo-Wiki](https://www.geo-wiki.org/) and [Collect Earth Online](https://collect.earth/home) are two open-source websites that may also be useful depending on the reference data strategy employed. Alternatively, there are many pre-existing training datasets on the web that may be useful, e.g. [Radiant Earth](https://www.radiant.earth/) manages a growing number of reference datasets for use by anyone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training data (feature layers) from the `open-data-cube` using geometries within a geojson. The default example will use the vegetated wetlands/non-vegetated wetland labels within the `'data/kenya_uganda_yearly_polygon_training_sites_{year}.geojson'` file. \n",
    "\n",
    "To do this, we rely on a custom `deafrica-sandbox-notebooks` function called `collect_training_data`, contained within the [deafrica_tools.classification](../../Tools/deafrica_tools/classification.py) script.  The principal goal of this notebook is to familarise users with this function so they can extract the appropriate data for their use-case. The default example also highlights extracting a set of useful feature layers for generating a vegetated wetland mask for Kenya and Uganda.\n",
    "\n",
    "\n",
    "1. Preview the polygons in our training data by plotting them on a basemap\n",
    "2. Define a feature layer function to pass to `collect_training_data`\n",
    "3. Extract training data from the datacube using `collect_training_data`\n",
    "4. Export the training data to disk for use in subsequent scripts\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.geometry import assign_crs\n",
    "\n",
    "from deafrica_tools.plotting import map_shapefile\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `year`: The year for which to get training data for. \n",
    "* `path`: The path to the input vector file from which we will extract training data. A default geojson is provided.\n",
    "* `field`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**.\n",
    "* `training_data`: The path to export the collected training data to.\n",
    "* `product` : Which Annual  GeoMAD product to use:\n",
    "    * Landsat 8 & 9 Annual GeoMAD ` gm_ls8_ls9_annual` is available from 2021- \n",
    "    * Sentinel-2 Annual GeoMAD `gm_s2_annual` is available from 2017 – 2020\n",
    "    * Landsat 8 Annual GeoMAD `gm_ls8_annual` is available from 2013 – 2020\n",
    "    * Landsat 5 & 7 Annual GeoMAD `gm_ls5_ls7_annual` is available from 1984 – 2012\n",
    "* `measurements` : The bands to load for the specified product. \n",
    "* `collection` : Used when calculating spectral indices from bands. `c2` for Landsat data and `s2` for Sentinel 2 data.\n",
    "* `resoluion` : The spatial resolution, in metres, to resample the satellite data to. `(-30, 30)` for Lansat data and `(-20, 20)` for Sentinel 2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2017\"\n",
    "path = f\"data/training_data/kenya_uganda_yearly_polygon_training_sites_{year}.geojson\"\n",
    "field = \"class\"\n",
    "\n",
    "# Create the output directory to store the results.\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Set the name and location of the output file.\n",
    "training_data = f\"{output_dir}/kenya_uganda_test_training_data_{year}.txt\"\n",
    "\n",
    "# Specify the product, measurements and resolution. \n",
    "product = \"gm_s2_annual\"\n",
    "measurements =  ['blue','green','red','nir','swir_1','swir_2','red_edge_1', 'red_edge_2', 'red_edge_3', 'BCMAD', 'EMAD', 'SMAD'] \n",
    "collection = \"s2\"\n",
    "resolution = (-20,20)\n",
    "\n",
    "# product = \"gm_ls5_ls7_annual\"\n",
    "# product = \"gm_ls8_annual\"\n",
    "# product = \" gm_ls8_ls9_annual\"\n",
    "# measurements =   ['blue','green','red','nir','swir_1','swir_2', 'BCMAD', 'EMAD', 'SMAD'] \n",
    "# collection = \"c2\"\n",
    "# resolution = (-30,30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 15\n"
     ]
    }
   ],
   "source": [
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview input data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class'). These labels will be used to train our model. \n",
    "\n",
    "> Remember, the class labels **must** be represented by `integers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>POLYGON ((39.14620 -0.37515, 39.14620 -0.37215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((35.74762 4.36950, 35.74762 4.37250, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>POLYGON ((40.40394 -0.52649, 40.40394 -0.52349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>POLYGON ((40.43746 0.53355, 40.43746 0.53655, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>POLYGON ((38.97078 -0.49255, 38.97078 -0.48955...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                           geometry\n",
       "0      9  POLYGON ((39.14620 -0.37515, 39.14620 -0.37215...\n",
       "1      4  POLYGON ((35.74762 4.36950, 35.74762 4.37250, ...\n",
       "2      9  POLYGON ((40.40394 -0.52649, 40.40394 -0.52349...\n",
       "3      9  POLYGON ((40.43746 0.53355, 40.43746 0.53655, ...\n",
       "4      9  POLYGON ((38.97078 -0.49255, 38.97078 -0.48955..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data shapefile\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# Plot first five rows\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d3b54c38a7484d97dfe906215e277c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dc00cce8474898adf6dc0a2a1306e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[0.17890070899999966, 35.625794355], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training data in an interactive map\n",
    "map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting training data\n",
    "\n",
    "The function `collect_training_data` takes our geojson containing class labels and extracts training data (features) from the datacube over the locations specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing any `NaN` or `inf` values.\n",
    "\n",
    "The below variables can be set within the `collect_training_data` function:\n",
    "\n",
    "* `zonal_stats`: An optional string giving the names of zonal statistics to calculate across each polygon (if the geometries in the vector file are polygons and not points). Default is None (all pixel values are returned). Supported values are 'mean', 'median', 'max', and 'min'. \n",
    "\n",
    "In addition to the `zonal_stats` parameter, we also need to set up a datacube query dictionary for the Open Data Cube query such as `measurements` (the bands to load from the satellite), the `resolution` (the cell size), and the `output_crs` (the output projection). These options will be added to a `query` dictionary that will be passed into `collect_training_data` using the parameter `collect_training_data(dc_query=query, ...)`.  The query dictionary will be the only argument in the **feature layer function** which we will define and describe in a moment.\n",
    "\n",
    "> Note: `collect_training_data` also has a number of additional parameters for handling ODC I/O read failures, where polygons that return an excessive number of null values can be resubmitted to the multiprocessing queue.  Check out the [docs](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/83116e80ebb4f8744e3de74e7a713aadd0a7577a/Tools/deafrica_tools/classification.py#L565) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = 'mean'\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "time = (year)\n",
    "\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a datacube query object from the parameters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'product': product,\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining feature layers\n",
    "\n",
    "To create the desired feature layers, we pass instructions to `collect_training_data` through the `feature_func` parameter.\n",
    "\n",
    "* `feature_func`: A function for generating feature layers that is applied to the data within the bounds of the input geometry. The `feature_func` must accept a `dc_query` dictionary, and return a single `xarray.Dataset` or `xarray.DataArray` containing 2D coordinates (i.e x, y - no time dimension). e.g.\n",
    "\n",
    "          def feature_function(query):\n",
    "              dc = datacube.Datacube(app='feature_layers')\n",
    "              ds = dc.load(**query)\n",
    "              ds = ds.mean('time')\n",
    "              return ds\n",
    "\n",
    "Below, we will define a more complicated feature layer function than the brief example shown above. We will calculate some band indices on the Sentinel-2 [geoMAD](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/master/Datasets/GeoMAD.ipynb) and append a slope dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "def feature_layers(query):\n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    #load Annual geomedian\n",
    "    ds = dc.load(**query)\n",
    "    \n",
    "    #calculate some band indices\n",
    "    da = calculate_indices(ds,\n",
    "                           index=['NDVI', 'LAI', 'MNDWI'],\n",
    "                           drop=False,\n",
    "                           collection=collection)\n",
    "    \n",
    "    #add slope dataset\n",
    "    url_slope = \"https://deafrica-input-datasets.s3.af-south-1.amazonaws.com/srtm_dem/srtm_africa_slope.tif\"\n",
    "    slope = rio_slurp_xarray(url_slope, gbox=ds.geobox)\n",
    "    slope = slope.to_dataset(name='slope')\n",
    "    \n",
    "    #merge results into single dataset \n",
    "    result = xr.merge([da, slope],compat='override')\n",
    "\n",
    "    return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the `collect_training_data` function.\n",
    "\n",
    "> **Note**: With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization. However, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization.  You can also limit the number of polygons to run when checking code. For example, passing in `gdf=input_data[0:5]` will only run the code over the first 5 polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking zonal statistic: mean\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55fba4a7dcf4d6dbdc0ed337f7a2635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 0.0 %\n",
      "Removed 0 rows wth NaNs &/or Infs\n",
      "Output shape:  (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "column_names, model_input = collect_training_data(\n",
    "                                    gdf=input_data,\n",
    "                                    dc_query=query,\n",
    "                                    ncpus=ncpus,\n",
    "                                    field=field,\n",
    "                                    zonal_stats=zonal_stats,\n",
    "                                    feature_func=feature_layers\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a list (`column_names`) contains a list of the names of the feature layers we've computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'blue', 'green', 'red', 'nir', 'swir_1', 'swir_2', 'red_edge_1', 'red_edge_2', 'red_edge_3', 'BCMAD', 'EMAD', 'SMAD', 'NDVI', 'LAI', 'MNDWI', 'slope']\n"
     ]
    }
   ],
   "source": [
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second object returned by the function is a numpy.array (`model_input`) and contains the data from our labelled geometries. The first item in the array is the class integer, the second set of items are the values for each feature layer we computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   9.   1225.5  1671.76 ...    0.34   -0.44    3.84]\n",
      " [   9.   1070.25 1377.49 ...    0.51   -0.46    2.6 ]\n",
      " [   9.   1166.82 1697.31 ...    0.26   -0.47    3.47]\n",
      " ...\n",
      " [   4.    531.18  768.35 ...    1.3    -0.48    5.81]\n",
      " [   9.    352.88  589.98 ...    1.51   -0.49   32.83]\n",
      " [   6.   1092.51 1708.37 ...    0.1    -0.47    4.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Once we've collected all the training data we require, we can write the data to disk. This will allow us to import the data in the next step(s) of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all columns\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names]\n",
    "#Export files to disk\n",
    "np.savetxt(training_data, model_input[:, model_col_indices], header=\" \".join(column_names), fmt=\"%4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended next steps\n",
    "\n",
    "To continue working through the notebooks in this `Scalable Machine Learning on the ODC` workflow, go to the next notebook `2_Inspect_training_data.ipynb`.\n",
    "\n",
    "1. **Extracting_training_data (this notebook)**\n",
    "2. [Inspect_training_data](2_Inspect_training_data.ipynb)\n",
    "3. [Evaluate_optimize_fit_classifier](3_Evaluate_optimize_fit_classifier.ipynb)\n",
    "4. [Classify_satellite_data](4_Classify_satellite_data.ipynb)\n",
    "5. [Object-based_filtering](5_Object-based_filtering.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.6\n"
     ]
    }
   ],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last Tested:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-02'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
