{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdac518-de9e-4e91-8375-ab4972b4e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import xarray as xr\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.cog import write_cog\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import geomedian_with_mads, xr_geomedian\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68117e-55a1-4e78-9dd3-8b95f2f7f381",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf6218-893f-454e-89d0-a3e4e70b3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))\n",
    "\n",
    "client = create_local_dask_cluster(return_client=True, n_workers=1, threads_per_worker=ncpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679589c0-2b8f-45fb-93e8-07c319a2d251",
   "metadata": {},
   "source": [
    "## Read in training data feaure names and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdfd9f-c15e-4482-84f5-8ecb3843d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names from fitted model data\n",
    "training_data = \"results/training_data.txt\"\n",
    "\n",
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, \"r\") as file:\n",
    "    header = file.readline()\n",
    "\n",
    "column_names = header.split()[2:]\n",
    "\n",
    "# Get label dictionary\n",
    "labels_path = \"results/class_labels.json\"\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c0efc-374c-476e-90a9-339a6ec4441d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load trained ML model and shapefile for prediction\n",
    "\n",
    "To manage memory, we provide a shapefile that splits the area of interest into tiles, which are then looped over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ec4d8-81bf-4186-8568-590b5d50de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and load\n",
    "model_path = \"results/randomforest_model.joblib\"\n",
    "model = load(model_path).set_params(n_jobs=1)\n",
    "\n",
    "# Choose file containing test areas and load\n",
    "districts_file = \"data/gridded_province.shp\"\n",
    "districts_gdf = gpd.read_file(districts_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087450e-ad5c-4017-81f6-f7e8985ddc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the query for running the predictions\n",
    "This uses the existing query from the training data collection notebook, and adds `dask_chunks` as an additional parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829adb4-d90a-4dbe-9119-0251cd79ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query used for fitting\n",
    "query_file = \"results/query.pickle\"\n",
    "\n",
    "with open(query_file, \"rb\") as f:\n",
    "    query = pickle.load(f)\n",
    "    \n",
    "# Specify any specific additions to the data query -- e.g. dask_chunks for enabling parallel computation\n",
    "dask_chunks = {\"x\": 4000, \"y\": 4000}\n",
    "query.update({\"dask_chunks\": dask_chunks})\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d60e-2c27-44f6-a5c6-b1903b8b5be8",
   "metadata": {},
   "source": [
    "## Run model over grids\n",
    "\n",
    "The model will be run for each area of the shapefile, producing a prediction file and a probabilities file. These will be saved to the data folder. The next notebook will then combine each separate file into a single raster map.\n",
    "\n",
    "If an area has already been processed, it will be skipped, and prediction will resume for any incomplete tiles. This is useful if the process fails partway through, or if you are logged out of the sandbox before completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e1527-c548-44ef-991e-b8963a659933",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest_gdf = districts_gdf\n",
    "district_column = \"id\"\n",
    "\n",
    "for index, district in area_of_interest_gdf.iterrows():\n",
    "    \n",
    "    # Set up geometry\n",
    "    district_name = str(int(district[district_column]))\n",
    "    print(f\"Processing {district_name}\")\n",
    "    \n",
    "    # Check if district has already been processed. If so, skip\n",
    "    output_filename = f\"data/district_{district_name}_croptype_prediction.tif\"\n",
    "    if os.path.exists(output_filename):\n",
    "        print(\"Completed; Skipping\")\n",
    "        continue\n",
    "\n",
    "    # set up query based on district polygon\n",
    "    geom = geometry.Geometry(geom=district.geometry, crs=area_of_interest_gdf.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load the feature data\n",
    "    print(\"    Loading feature data\")\n",
    "    data = feature_layers(query)\n",
    "    \n",
    "    # Only keep features that are in the original list of columns \n",
    "    # (currently needed to fix an issue where S1 is available in some regions of Zambia in March/April 2022, but not all)\n",
    "    data_cols = list(data.keys())\n",
    "    cols_to_drop = [column for column in data_cols if column not in column_names]\n",
    "    data = data.drop(cols_to_drop)\n",
    "\n",
    "    #predict using the imported model\n",
    "    predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           proba=True,\n",
    "                           persist=True,\n",
    "                           clean=True,\n",
    "                           return_input=False\n",
    "                          ).astype(np.uint8).persist()\n",
    "    \n",
    "    # Load masks and clip\n",
    "    crop_mask_query = query.copy()\n",
    "    crop_mask_query.update({\"time\": \"2019\"})\n",
    "\n",
    "    # Load the crop mask\n",
    "    print(\"    Loading crop_mask\")\n",
    "    crop_mask = dc.load(product=\"crop_mask\", **crop_mask_query)\n",
    "    \n",
    "    # Create a mask for the district\n",
    "    print(\"    Getting district mask\")\n",
    "    district_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"DISTRICT\": [district_name], \"geometry\": [district.geometry]}, crs=area_of_interest_gdf.crs),\n",
    "        da=predicted,\n",
    "        transform=predicted.geobox.transform,\n",
    "        crs=\"EPSG:6933\",\n",
    "    )\n",
    "\n",
    "    # set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions to\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "        predicted.Predictions.where((crop_mask.filtered == 1) & (district_mask==1), NODATA)\n",
    "    ).compute()\n",
    "    \n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    # Write to cog\n",
    "    prediction_file = f\"data/district_{district_name}_croptype_prediction.tif\"\n",
    "    print(f\"    Writing predictions to {prediction_file}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=prediction_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del predicted_masked\n",
    "    \n",
    "    probability_masked = (\n",
    "        predicted.Probabilities.where((crop_mask.filtered == 1) & (district_mask==1), NODATA)\n",
    "    ).compute()\n",
    "    \n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    probabilities_file = f\"data/district_{district_name}_croptype_probabilities.tif\"\n",
    "    print(f\"    Writing probabilities to {probabilities_file}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del probability_masked\n",
    "    \n",
    "    del crop_mask\n",
    "    del district_mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab671218-6ee9-42e4-a95a-b9ee3fa6fb9d",
   "metadata": {},
   "source": [
    "## Close the dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4afe1-1901-4023-a41a-1230d6e84711",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
