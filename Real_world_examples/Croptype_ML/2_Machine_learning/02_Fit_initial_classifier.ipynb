{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565e121-2001-4b0e-9789-f6eb1e15300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, balanced_accuracy_score, f1_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d80f6-4b05-48df-908e-572b0d468304",
   "metadata": {},
   "source": [
    "## Read in training data and label dictionary\n",
    "\n",
    "### Define the data and label paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1111f-41c0-456b-a175-304b6eb3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data file from previous step\n",
    "data_path = \"results/training_data_mlfeatureeng.txt\"\n",
    "\n",
    "# Dictionary with class labels from previous step\n",
    "labels_path = \"results/class_labels.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d203363-3d9e-48d9-90a7-8622a4302845",
   "metadata": {},
   "source": [
    "### Load the data and identify the feature columns for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda5dd0-f934-48c4-a711-0e25b642858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(data_path)\n",
    "\n",
    "# load the column_names\n",
    "with open(data_path, \"r\") as file:\n",
    "    header = file.readline()\n",
    "\n",
    "# Remove comment symbol from header, then extract label and feature names\n",
    "column_names = header.split()[1:]\n",
    "\n",
    "label_col = column_names[0]\n",
    "feature_cols = column_names[1:]\n",
    "\n",
    "print(f\"Label column:\\n{label_col}\\n\")\n",
    "print(f\"Feature columns:\\n{feature_cols}\\n\")\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]\n",
    "\n",
    "# Read the class label dictionary\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3188ad6-7c03-4856-b118-4731dbec8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6f785-6bb6-4726-bd8f-f241323e867b",
   "metadata": {},
   "source": [
    "## Convert model input into sklearn format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e54bf2-2fb4-4e74-b675-e7797788b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into a Pandas DataFrame, then split into features and labels\n",
    "model_input_df = pd.DataFrame(model_input, columns=column_names)\n",
    "X = model_input_df.drop(label_col, axis=1).values\n",
    "y = model_input_df[[label_col]].values.ravel()\n",
    "\n",
    "# Investigate value counts for each class\n",
    "model_input_df[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b5b25-65b3-4db9-ac44-fa67e730aaaf",
   "metadata": {},
   "source": [
    "## Fit, tune and evaluate multiple models using nested cross-validation\n",
    "\n",
    "This step allows us to train and tune mutliple models on fixed subsets of our data.\n",
    "\n",
    "When performing cross validation, data is split into `n` folds. One fold is kept aside as test data, and the rest is used to train a model. This step is repeated until each fold has been used as a test set, having been trained on the other two. From each fold, we get an estimate of the performance, which can be averaged to understand expected performance of a model on unseen data.\n",
    "\n",
    "Nested cross-validation introduces an additional step. Each set of training data is split into `m` further folds, and one is kept aside as test data specifically for fitting hyperparameters. The best parameters identifed across the `m` folds are then passed to the performance estimation folds.\n",
    "\n",
    "These steps are shown in the image below, with the larger green folds showing the performance step with `n=3` folds, and the hyperparameter tuning step with `m=4` folds.\n",
    "\n",
    "<img align=\"center\" src=\"../../../Supplementary_data/Scalable_machine_learning/nested_CV.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5745827-ca55-476b-bd89-eaa0acb20a8c",
   "metadata": {},
   "source": [
    "### Get number of cpus available for nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8f53f-12c6-49e3-b3b9-b73d79eb8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a6282-7190-465a-9aee-b790f0326cf7",
   "metadata": {},
   "source": [
    "### Construct the models and their parameter grids for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e700b97-fe74-406e-a3ed-a5895b956203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store models\n",
    "models = []\n",
    "\n",
    "\n",
    "# Random forest grid and model\n",
    "model_name = \"RandomForest\"\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"max_features\": [\"auto\", \"log2\", None],\n",
    "    \"n_estimators\": [200, 300, 400],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "models.append((model_name, RandomForestClassifier(n_jobs=1), rf_param_grid))\n",
    "\n",
    "# Ada Boost grid and model\n",
    "model_name = \"AdaBoost\"\n",
    "\n",
    "ab_param_grid = {\n",
    "    \"base_estimator\": [DecisionTreeClassifier(max_depth=i) for i in [1, 3, 10]],\n",
    "    \"n_estimators\": [10, 100, 1000],\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "models.append((model_name, AdaBoostClassifier(), ab_param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba41e89-3817-4d1e-ab0d-1ec66b44d9b5",
   "metadata": {},
   "source": [
    "### Perform nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f3a19-27e9-4543-8531-3ce345f8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose performance metrics\n",
    "metrics = [\"balanced_accuracy\", \"f1_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a8a8f-eb85-4086-8595-5dd89fa1d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store outputs\n",
    "results = {}\n",
    "test_indices = []\n",
    "\n",
    "# Only run a single trial for each algorithm, so set a single seed to use for selecting folds\n",
    "cv_seed = 13\n",
    "model_seed = 32\n",
    "\n",
    "# Set number of splits to do\n",
    "inner_cv_splits = 3\n",
    "outer_cv_splits = 3\n",
    "\n",
    "# Number of jobs to pass to the inner cross validation loop\n",
    "n_jobs_outer = 3\n",
    "n_jobs_inner = ncpus - n_jobs_outer\n",
    "\n",
    "for name, model, p_grid in models:\n",
    "    print(f\"Running {name}\")\n",
    "\n",
    "    # Construct the inner cross validation strategy and activity (GridSearchCV)\n",
    "    inner_cv = StratifiedKFold(\n",
    "        n_splits=inner_cv_splits, shuffle=True, random_state=cv_seed\n",
    "    )\n",
    "\n",
    "    # Store test sets for use with\n",
    "    for train_idx, test_idx in inner_cv.split(X, y):\n",
    "        test_indices.append(test_idx)\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=p_grid,\n",
    "        scoring=\"f1_weighted\",\n",
    "        cv=inner_cv,\n",
    "        n_jobs=n_jobs_inner,\n",
    "    )\n",
    "\n",
    "    # Construct the outer cross validation stategy and activity (cross_validate)\n",
    "    # Request that cross_validate returns the estimator\n",
    "    outer_cv = StratifiedKFold(\n",
    "        n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed\n",
    "    )\n",
    "    scores_array = cross_validate(\n",
    "        clf,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=outer_cv,\n",
    "        scoring=metrics,\n",
    "        return_estimator=True,\n",
    "        n_jobs=n_jobs_outer,\n",
    "    )\n",
    "\n",
    "    for metric in metrics:\n",
    "        mean = scores_array[f\"test_{metric}\"].mean()\n",
    "        std = scores_array[f\"test_{metric}\"].std()\n",
    "\n",
    "        print(f\"Average {metric} = {mean:.2f} with std. dev. of {std:.2f}.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Add the best model and best parameters for each outer split\n",
    "    results[f\"{name}_best_estimators\"] = [\n",
    "        scores_array[\"estimator\"][i].best_estimator_\n",
    "        for i in range(len(scores_array[\"estimator\"]))\n",
    "    ]\n",
    "    results[f\"{name}_best_parameters\"] = [\n",
    "        scores_array[\"estimator\"][i].best_params_\n",
    "        for i in range(len(scores_array[\"estimator\"]))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b90f2d-5181-4aa7-bd88-18d27b68af23",
   "metadata": {},
   "source": [
    "## Investigate results\n",
    "\n",
    "One of the most useful ways to get insight into the performance of machine learning classifiers is to view the confusion matrix, which counts the number of points in each predicted class as a function of the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ade559-cf68-40cb-a3d3-363d28c70dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model, p_grid in models:\n",
    "\n",
    "    clf_list = results[f\"{name}_best_estimators\"]\n",
    "    val_file = f\"results/{name}_confusionmat_values_mlfeatureeng.png\"\n",
    "    norm_file = f\"results/{name}_confusionmat_normalised_mlfeatureeng.png\"\n",
    "\n",
    "    val_fig, val_ax = plt.subplots(1, inner_cv_splits, figsize=(8 * inner_cv_splits, 7))\n",
    "    val_fig.suptitle(f\"{name} Confusion Matrix (Value)\")\n",
    "    norm_fig, norm_ax = plt.subplots(\n",
    "        1, inner_cv_splits, figsize=(8 * inner_cv_splits, 7)\n",
    "    )\n",
    "    norm_fig.suptitle(f\"{name} Confusion Matrix (Row Normalised)\", fontsize=16)\n",
    "\n",
    "    for i in range(inner_cv_splits):\n",
    "        clf = clf_list[i]\n",
    "        test_idx = test_indices[i]\n",
    "        X_test = X[test_idx, :]\n",
    "        y_test = y[test_idx]\n",
    "\n",
    "        # Plot unormalised confusion matrix\n",
    "        val_ax[i].set_title(f\"Fold {i}\")\n",
    "        ConfusionMatrixDisplay.from_estimator(\n",
    "            clf,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            normalize=None,\n",
    "            ax=val_ax[i],\n",
    "            colorbar=False,\n",
    "            display_labels=list(labels_dict.keys()),\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "\n",
    "        val_fig.savefig(val_file, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "\n",
    "        # Plot normalised confusion matrix\n",
    "        norm_ax[i].set_title(f\"Fold {i}\")\n",
    "        ConfusionMatrixDisplay.from_estimator(\n",
    "            clf,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            normalize=\"true\",\n",
    "            ax=norm_ax[i],\n",
    "            colorbar=False,\n",
    "            display_labels=list(labels_dict.keys()),\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "\n",
    "        norm_fig.savefig(norm_file, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b1367-7056-498c-8579-939eac90a01a",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with outer folds\n",
    "\n",
    "After running nested cross-validation, we have selected the best model and understood the performance we can expect to see on new data.\n",
    "\n",
    "Once the model is decided, we can run hyperparameter tuning using the outer fold only, allowing us to tune the model with additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc671b6-34e1-467a-9af1-aa484b6ebeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimated params for RF model\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed)\n",
    "\n",
    "metric = \"f1_macro\"\n",
    "\n",
    "# instatiate a gridsearchCV using outer cross-validation folds\n",
    "clf = GridSearchCV(\n",
    "    RandomForestClassifier(n_jobs=1),\n",
    "    rf_param_grid,\n",
    "    scoring=metric,\n",
    "    verbose=1,\n",
    "    cv=outer_cv.split(X, y),\n",
    "    n_jobs=ncpus,\n",
    ")\n",
    "\n",
    "# Fit the gridsearch on outer cross-validation folds\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"The \" + metric + \" score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cac00e-5fd8-469c-af65-1a6ec9e0b043",
   "metadata": {},
   "source": [
    "## Final model fit\n",
    "\n",
    "The cross-validation steps have allowed us to pick the best performing model on unseen data and further tune that model. The final step is to fit the model to all of the data, using the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9731e9-4532-40c4-a731-e78282affd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model\n",
    "new_model = RandomForestClassifier(**clf.best_params_, random_state=1, n_jobs=ncpus)\n",
    "new_model.fit(X, y)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "\n",
    "# Export the final model for use in following notebooks\n",
    "dump(new_model, \"results/randomforest_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
